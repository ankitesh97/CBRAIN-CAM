{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbrain.imports import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.normalization import *\n",
    "import h5py\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorClassification(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data_fn, input_vars, output_vars, percentile_path, data_name,\n",
    "                 norm_fn=None, input_transform=None, output_transform=None,\n",
    "                 batch_size=1024, shuffle=True, xarray=False, var_cut_off=None, normalize_flag=True, bin_size=100):\n",
    "        # Just copy over the attributes\n",
    "        self.data_fn, self.norm_fn = data_fn, norm_fn\n",
    "        self.input_vars, self.output_vars = input_vars, output_vars\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "        self.bin_size = bin_size\n",
    "        self.percentile_bins = load_pickle(percentile_path)['Percentile'][data_name]\n",
    "        self.enc = OneHotEncoder(sparse=False)\n",
    "        classes = np.arange(self.bin_size+2)\n",
    "        self.enc.fit(classes.reshape(-1,1))\n",
    "        # Open datasets\n",
    "        self.data_ds = xr.open_dataset(data_fn)\n",
    "        if norm_fn is not None: self.norm_ds = xr.open_dataset(norm_fn)\n",
    "     # Compute number of samples and batches\n",
    "        self.n_samples = self.data_ds.vars.shape[0]\n",
    "        self.n_batches = int(np.floor(self.n_samples) / self.batch_size)\n",
    "\n",
    "        # Get input and output variable indices\n",
    "        self.input_idxs = return_var_idxs(self.data_ds, input_vars, var_cut_off)\n",
    "        self.output_idxs = return_var_idxs(self.data_ds, output_vars)\n",
    "        self.n_inputs, self.n_outputs = len(self.input_idxs), len(self.output_idxs)\n",
    "        \n",
    "                # Initialize input and output normalizers/transformers\n",
    "        if input_transform is None:\n",
    "            self.input_transform = Normalizer()\n",
    "        elif type(input_transform) is tuple:\n",
    "            ## normalize flag added by Ankitesh\n",
    "            self.input_transform = InputNormalizer(\n",
    "                self.norm_ds,normalize_flag, input_vars, input_transform[0], input_transform[1], var_cut_off)\n",
    "        else:\n",
    "            self.input_transform = input_transform  # Assume an initialized normalizer is passed\n",
    "            \n",
    "            \n",
    "        if output_transform is None:\n",
    "            self.output_transform = Normalizer()\n",
    "        elif type(output_transform) is dict:\n",
    "            self.output_transform = DictNormalizer(self.norm_ds, output_vars, output_transform)\n",
    "        else:\n",
    "            self.output_transform = output_transform  # Assume an initialized normalizer is passed\n",
    "\n",
    "        # Now close the xarray file and load it as an h5 file instead\n",
    "        # This significantly speeds up the reading of the data...\n",
    "        if not xarray:\n",
    "            self.data_ds.close()\n",
    "            self.data_ds = h5py.File(data_fn, 'r')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "    \n",
    "    # TODO: Find a better way to implement this, currently it is the hardcoded way.\n",
    "    def _transform_to_one_hot(self,Y):\n",
    "        '''\n",
    "            return shape = batch_size X 64 X bin_size\n",
    "        '''\n",
    "\n",
    "        Y_trans = []\n",
    "        out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "        var_dict = {}\n",
    "        var_dict['PHQ'] = Y[:,:30]\n",
    "        var_dict['TPHYSTND'] = Y[:,30:60]\n",
    "        var_dict['FSNT'] = Y[:,60]\n",
    "        var_dict['FSNS'] = Y[:,61]\n",
    "        var_dict['FLNT'] = Y[:,62]\n",
    "        var_dict['FLNS'] = Y[:,63]\n",
    "        perc = self.percentile_bins\n",
    "        for var in out_vars[:2]:\n",
    "            all_levels_one_hot = []\n",
    "            for ilev in range(30):\n",
    "                bin_index = np.digitize(var_dict[var][:,ilev],perc[var][ilev])\n",
    "                one_hot = self.enc.transform(bin_index.reshape(-1,1))\n",
    "                all_levels_one_hot.append(one_hot)\n",
    "            var_one_hot = np.stack(all_levels_one_hot,axis=1) \n",
    "            Y_trans.append(var_one_hot)\n",
    "        for var in out_vars[2:]:\n",
    "            bin_index = np.digitize(var_dict[var][:], perc[var])\n",
    "            one_hot = self.enc.transform(bin_index.reshape(-1,1))[:,np.newaxis,:]\n",
    "            Y_trans.append(one_hot)\n",
    "        return np.concatenate(Y_trans,axis=1)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "\n",
    "        # Normalize\n",
    "        X = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y) #shape batch_size X 64 \n",
    "        Y = self._transform_to_one_hot(Y)\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(self.n_batches)\n",
    "        if self.shuffle: np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = load_pickle('/export/nfs0home/ankitesg/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = 'CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "data_path = '/scratch/ankitesh/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = DataGeneratorClassification(\n",
    "    data_fn=f'{data_path}{TRAINFILE}', \n",
    "    input_vars= ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX'], \n",
    "    output_vars=['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS'], \n",
    "    percentile_path='/export/nfs0home/ankitesg/data/percentile_data.pkl', \n",
    "    data_name = 'M4K',\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    norm_fn = f'{data_path}{NORMFILE}',\n",
    "    batch_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 64, 102)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CBrainEnv",
   "language": "python",
   "name": "cbrainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
