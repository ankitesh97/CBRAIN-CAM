{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,\"/home1/07064/tg863631/anaconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages\") #work around for h5py\n",
    "from cbrain.imports import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "# import tensorflow_probability as tfp\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "import matplotlib.ticker as mticker\n",
    "import pickle\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import datetime\n",
    "from cbrain.climate_invariant import *\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbrain.imports import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.normalization import *\n",
    "import h5py\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorClassification(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data_fn, input_vars, output_vars, percentile_path, data_name,\n",
    "                 norm_fn=None, input_transform=None, output_transform=None,\n",
    "                 batch_size=1024, shuffle=True, xarray=False, var_cut_off=None, normalize_flag=True, bin_size=100):\n",
    "        # Just copy over the attributes\n",
    "        self.data_fn, self.norm_fn = data_fn, norm_fn\n",
    "        self.input_vars, self.output_vars = input_vars, output_vars\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "        self.bin_size = bin_size\n",
    "        self.percentile_bins = load_pickle(percentile_path)['Percentile'][data_name]\n",
    "        self.enc = OneHotEncoder(sparse=False)\n",
    "        classes = np.arange(self.bin_size+2)\n",
    "        self.enc.fit(classes.reshape(-1,1))\n",
    "        # Open datasets\n",
    "        self.data_ds = xr.open_dataset(data_fn)\n",
    "        if norm_fn is not None: self.norm_ds = xr.open_dataset(norm_fn)\n",
    "     # Compute number of samples and batches\n",
    "        self.n_samples = self.data_ds.vars.shape[0]\n",
    "        self.n_batches = int(np.floor(self.n_samples) / self.batch_size)\n",
    "\n",
    "        # Get input and output variable indices\n",
    "        self.input_idxs = return_var_idxs(self.data_ds, input_vars, var_cut_off)\n",
    "        self.output_idxs = return_var_idxs(self.data_ds, output_vars)\n",
    "        self.n_inputs, self.n_outputs = len(self.input_idxs), len(self.output_idxs)\n",
    "        \n",
    "                # Initialize input and output normalizers/transformers\n",
    "        if input_transform is None:\n",
    "            self.input_transform = Normalizer()\n",
    "        elif type(input_transform) is tuple:\n",
    "            ## normalize flag added by Ankitesh\n",
    "            self.input_transform = InputNormalizer(\n",
    "                self.norm_ds,normalize_flag, input_vars, input_transform[0], input_transform[1], var_cut_off)\n",
    "        else:\n",
    "            self.input_transform = input_transform  # Assume an initialized normalizer is passed\n",
    "            \n",
    "            \n",
    "        if output_transform is None:\n",
    "            self.output_transform = Normalizer()\n",
    "        elif type(output_transform) is dict:\n",
    "            self.output_transform = DictNormalizer(self.norm_ds, output_vars, output_transform)\n",
    "        else:\n",
    "            self.output_transform = output_transform  # Assume an initialized normalizer is passed\n",
    "\n",
    "        # Now close the xarray file and load it as an h5 file instead\n",
    "        # This significantly speeds up the reading of the data...\n",
    "        if not xarray:\n",
    "            self.data_ds.close()\n",
    "            self.data_ds = h5py.File(data_fn, 'r')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "    \n",
    "    # TODO: Find a better way to implement this, currently it is the hardcoded way.\n",
    "    def _transform_to_one_hot(self,Y):\n",
    "        '''\n",
    "            return shape = batch_size X 64 X bin_size\n",
    "        '''\n",
    "\n",
    "        Y_trans = []\n",
    "        out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "        var_dict = {}\n",
    "        var_dict['PHQ'] = Y[:,:30]\n",
    "        var_dict['TPHYSTND'] = Y[:,30:60]\n",
    "        var_dict['FSNT'] = Y[:,60]\n",
    "        var_dict['FSNS'] = Y[:,61]\n",
    "        var_dict['FLNT'] = Y[:,62]\n",
    "        var_dict['FLNS'] = Y[:,63]\n",
    "        perc = self.percentile_bins\n",
    "        for var in out_vars[:2]:\n",
    "            all_levels_one_hot = []\n",
    "            for ilev in range(30):\n",
    "                bin_index = np.digitize(var_dict[var][:,ilev],perc[var][ilev])\n",
    "                one_hot = self.enc.transform(bin_index.reshape(-1,1))\n",
    "                all_levels_one_hot.append(one_hot)\n",
    "            var_one_hot = np.stack(all_levels_one_hot,axis=1) \n",
    "            Y_trans.append(var_one_hot)\n",
    "        for var in out_vars[2:]:\n",
    "            bin_index = np.digitize(var_dict[var][:], perc[var])\n",
    "            one_hot = self.enc.transform(bin_index.reshape(-1,1))[:,np.newaxis,:]\n",
    "            Y_trans.append(one_hot)\n",
    "        \n",
    "        Y_concatenated = np.concatenate(Y_trans,axis=1)\n",
    "        transformed = {}\n",
    "        for i in range(64):\n",
    "            transformed[f'output_{i}'] = Y_concatenated[:,i,:]\n",
    "        return transformed\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "        # Normalize\n",
    "        X = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y) #shape batch_size X 64 \n",
    "        Y = self._transform_to_one_hot(Y)\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(self.n_batches)\n",
    "        if self.shuffle: np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = load_pickle('/export/nfs0home/ankitesg/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = 'CI_SP_M4K_train_shuffle.nc'\n",
    "VALIDFILE = 'CI_SP_M4K_valid.nc'\n",
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "data_path = '/scratch/ankitesh/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGeneratorClassification(\n",
    "    data_fn=f'{data_path}{TRAINFILE}', \n",
    "    input_vars=['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX'], \n",
    "    output_vars=['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS'], \n",
    "    percentile_path='/export/nfs0home/ankitesg/data/percentile_data.pkl', \n",
    "    data_name = 'M4K',\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    norm_fn = f'{data_path}{NORMFILE}',\n",
    "    batch_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen = DataGeneratorClassification(\n",
    "    data_fn=f'{data_path}{VALIDFILE}', \n",
    "    input_vars=['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX'], \n",
    "    output_vars=['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS'], \n",
    "    percentile_path='/export/nfs0home/ankitesg/data/percentile_data.pkl', \n",
    "    data_name = 'M4K',\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    norm_fn = f'{data_path}{NORMFILE}',\n",
    "    batch_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (mult-output classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this defines a single branch out of 64 branches\n",
    "def define_single_output_branch(densout,out_index):\n",
    "    out = Dense(bin_size+2, activation='softmax',name=f\"output_{out_index}\")(densout)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,))\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "densout = Dense(32, activation='linear')(densout)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "all_outputs = [define_single_output_branch(densout,i) for i in range(64)]\n",
    "model = tf.keras.models.Model(inputs=inp, outputs=all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          8320        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          16512       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           4128        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 32)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_0 (Dense)                (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_1 (Dense)                (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_2 (Dense)                (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_3 (Dense)                (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_4 (Dense)                (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_5 (Dense)                (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_6 (Dense)                (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_7 (Dense)                (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_8 (Dense)                (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_9 (Dense)                (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_10 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_11 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_12 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_13 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_14 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_15 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_16 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_17 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_18 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_19 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_20 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_21 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_22 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_23 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_24 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_25 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_26 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_27 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_28 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_29 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_30 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_31 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_32 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_33 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_34 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_35 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_36 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_37 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_38 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_39 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_40 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_41 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_42 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_43 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_44 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_45 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_46 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_47 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_48 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_49 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_50 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_51 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_52 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_53 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_54 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_55 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_56 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_57 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_58 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_59 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_60 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_61 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_62 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_63 (Dense)               (None, 102)          3366        leaky_re_lu_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 293,920\n",
      "Trainable params: 293,920\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "for i in range(64):\n",
    "    losses[f'output_{i}'] = \"categorical_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=losses, metrics=[\"accuracy\"])\n",
    "path_HDF5 = '/scratch/ankitesh/models/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+'BF_Classification.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "41376/41376 [==============================] - 33697s 814ms/step - loss: 233.5987 - output_0_loss: 0.0043 - output_1_loss: 0.0042 - output_2_loss: 4.0976 - output_3_loss: 4.1669 - output_4_loss: 4.4971 - output_5_loss: 4.4752 - output_6_loss: 4.3806 - output_7_loss: 4.2654 - output_8_loss: 4.0766 - output_9_loss: 3.9340 - output_10_loss: 3.9258 - output_11_loss: 3.9577 - output_12_loss: 3.9282 - output_13_loss: 3.8840 - output_14_loss: 3.8664 - output_15_loss: 3.8917 - output_16_loss: 3.9419 - output_17_loss: 3.9711 - output_18_loss: 3.9738 - output_19_loss: 3.9821 - output_20_loss: 4.0074 - output_21_loss: 4.0448 - output_22_loss: 4.1004 - output_23_loss: 4.1583 - output_24_loss: 4.1834 - output_25_loss: 4.2201 - output_26_loss: 4.2596 - output_27_loss: 4.2870 - output_28_loss: 4.3016 - output_29_loss: 4.2966 - output_30_loss: 2.0842 - output_31_loss: 2.2168 - output_32_loss: 3.9311 - output_33_loss: 3.9611 - output_34_loss: 2.7627 - output_35_loss: 2.8475 - output_36_loss: 3.2586 - output_37_loss: 3.5441 - output_38_loss: 3.6028 - output_39_loss: 3.6052 - output_40_loss: 3.6741 - output_41_loss: 3.6901 - output_42_loss: 3.6571 - output_43_loss: 3.6448 - output_44_loss: 3.6867 - output_45_loss: 3.6788 - output_46_loss: 3.6481 - output_47_loss: 3.5855 - output_48_loss: 3.5066 - output_49_loss: 3.5058 - output_50_loss: 3.5770 - output_51_loss: 3.7246 - output_52_loss: 3.9496 - output_53_loss: 4.1905 - output_54_loss: 4.3028 - output_55_loss: 4.3697 - output_56_loss: 4.3707 - output_57_loss: 4.3182 - output_58_loss: 4.2605 - output_59_loss: 4.3295 - output_60_loss: 1.0007 - output_61_loss: 1.0950 - output_62_loss: 3.1857 - output_63_loss: 3.7486 - output_0_accuracy: 0.9997 - output_1_accuracy: 0.9997 - output_2_accuracy: 0.0771 - output_3_accuracy: 0.0646 - output_4_accuracy: 0.0224 - output_5_accuracy: 0.0216 - output_6_accuracy: 0.0233 - output_7_accuracy: 0.0275 - output_8_accuracy: 0.0375 - output_9_accuracy: 0.0462 - output_10_accuracy: 0.0412 - output_11_accuracy: 0.0402 - output_12_accuracy: 0.0448 - output_13_accuracy: 0.0503 - output_14_accuracy: 0.0557 - output_15_accuracy: 0.0585 - output_16_accuracy: 0.0594 - output_17_accuracy: 0.0613 - output_18_accuracy: 0.0613 - output_19_accuracy: 0.0570 - output_20_accuracy: 0.0514 - output_21_accuracy: 0.0473 - output_22_accuracy: 0.0428 - output_23_accuracy: 0.0381 - output_24_accuracy: 0.0358 - output_25_accuracy: 0.0341 - output_26_accuracy: 0.0320 - output_27_accuracy: 0.0311 - output_28_accuracy: 0.0299 - output_29_accuracy: 0.0287 - output_30_accuracy: 0.2887 - output_31_accuracy: 0.2462 - output_32_accuracy: 0.0489 - output_33_accuracy: 0.0470 - output_34_accuracy: 0.1306 - output_35_accuracy: 0.1234 - output_36_accuracy: 0.0908 - output_37_accuracy: 0.0770 - output_38_accuracy: 0.0710 - output_39_accuracy: 0.0696 - output_40_accuracy: 0.0669 - output_41_accuracy: 0.0695 - output_42_accuracy: 0.0710 - output_43_accuracy: 0.0713 - output_44_accuracy: 0.0678 - output_45_accuracy: 0.0704 - output_46_accuracy: 0.0751 - output_47_accuracy: 0.0818 - output_48_accuracy: 0.0893 - output_49_accuracy: 0.0890 - output_50_accuracy: 0.0812 - output_51_accuracy: 0.0697 - output_52_accuracy: 0.0545 - output_53_accuracy: 0.0405 - output_54_accuracy: 0.0350 - output_55_accuracy: 0.0321 - output_56_accuracy: 0.0316 - output_57_accuracy: 0.0335 - output_58_accuracy: 0.0351 - output_59_accuracy: 0.0323 - output_60_accuracy: 0.6371 - output_61_accuracy: 0.6139 - output_62_accuracy: 0.0990 - output_63_accuracy: 0.0667 - val_loss: 229.3129 - val_output_0_loss: 1.1948e-07 - val_output_1_loss: 1.1948e-07 - val_output_2_loss: 4.0571 - val_output_3_loss: 4.1189 - val_output_4_loss: 4.4832 - val_output_5_loss: 4.4665 - val_output_6_loss: 4.3671 - val_output_7_loss: 4.2476 - val_output_8_loss: 4.0560 - val_output_9_loss: 3.8983 - val_output_10_loss: 3.8988 - val_output_11_loss: 3.9369 - val_output_12_loss: 3.9020 - val_output_13_loss: 3.8551 - val_output_14_loss: 3.8317 - val_output_15_loss: 3.8512 - val_output_16_loss: 3.8966 - val_output_17_loss: 3.9246 - val_output_18_loss: 3.9168 - val_output_19_loss: 3.9258 - val_output_20_loss: 3.9488 - val_output_21_loss: 3.9887 - val_output_22_loss: 4.0480 - val_output_23_loss: 4.1144 - val_output_24_loss: 4.1428 - val_output_25_loss: 4.1820 - val_output_26_loss: 4.2248 - val_output_27_loss: 4.2556 - val_output_28_loss: 4.2755 - val_output_29_loss: 4.2725 - val_output_30_loss: 1.8034 - val_output_31_loss: 1.9703 - val_output_32_loss: 3.9024 - val_output_33_loss: 3.9328 - val_output_34_loss: 2.5360 - val_output_35_loss: 2.7161 - val_output_36_loss: 3.1602 - val_output_37_loss: 3.4741 - val_output_38_loss: 3.5464 - val_output_39_loss: 3.5416 - val_output_40_loss: 3.6095 - val_output_41_loss: 3.6165 - val_output_42_loss: 3.5814 - val_output_43_loss: 3.5636 - val_output_44_loss: 3.5933 - val_output_45_loss: 3.5713 - val_output_46_loss: 3.5228 - val_output_47_loss: 3.4609 - val_output_48_loss: 3.3750 - val_output_49_loss: 3.3765 - val_output_50_loss: 3.4738 - val_output_51_loss: 3.6414 - val_output_52_loss: 3.8885 - val_output_53_loss: 4.1471 - val_output_54_loss: 4.2651 - val_output_55_loss: 4.3351 - val_output_56_loss: 4.3345 - val_output_57_loss: 4.2734 - val_output_58_loss: 4.2061 - val_output_59_loss: 4.2810 - val_output_60_loss: 0.8848 - val_output_61_loss: 0.9920 - val_output_62_loss: 3.0275 - val_output_63_loss: 3.6209 - val_output_0_accuracy: 1.0000 - val_output_1_accuracy: 1.0000 - val_output_2_accuracy: 0.0823 - val_output_3_accuracy: 0.0685 - val_output_4_accuracy: 0.0225 - val_output_5_accuracy: 0.0218 - val_output_6_accuracy: 0.0239 - val_output_7_accuracy: 0.0289 - val_output_8_accuracy: 0.0395 - val_output_9_accuracy: 0.0494 - val_output_10_accuracy: 0.0437 - val_output_11_accuracy: 0.0425 - val_output_12_accuracy: 0.0468 - val_output_13_accuracy: 0.0522 - val_output_14_accuracy: 0.0583 - val_output_15_accuracy: 0.0617 - val_output_16_accuracy: 0.0625 - val_output_17_accuracy: 0.0645 - val_output_18_accuracy: 0.0664 - val_output_19_accuracy: 0.0621 - val_output_20_accuracy: 0.0564 - val_output_21_accuracy: 0.0509 - val_output_22_accuracy: 0.0458 - val_output_23_accuracy: 0.0408 - val_output_24_accuracy: 0.0387 - val_output_25_accuracy: 0.0371 - val_output_26_accuracy: 0.0348 - val_output_27_accuracy: 0.0337 - val_output_28_accuracy: 0.0321 - val_output_29_accuracy: 0.0307 - val_output_30_accuracy: 0.3437 - val_output_31_accuracy: 0.2898 - val_output_32_accuracy: 0.0502 - val_output_33_accuracy: 0.0486 - val_output_34_accuracy: 0.1597 - val_output_35_accuracy: 0.1407 - val_output_36_accuracy: 0.1021 - val_output_37_accuracy: 0.0847 - val_output_38_accuracy: 0.0776 - val_output_39_accuracy: 0.0757 - val_output_40_accuracy: 0.0722 - val_output_41_accuracy: 0.0761 - val_output_42_accuracy: 0.0766 - val_output_43_accuracy: 0.0782 - val_output_44_accuracy: 0.0737 - val_output_45_accuracy: 0.0783 - val_output_46_accuracy: 0.0843 - val_output_47_accuracy: 0.0911 - val_output_48_accuracy: 0.1006 - val_output_49_accuracy: 0.1002 - val_output_50_accuracy: 0.0895 - val_output_51_accuracy: 0.0750 - val_output_52_accuracy: 0.0578 - val_output_53_accuracy: 0.0436 - val_output_54_accuracy: 0.0379 - val_output_55_accuracy: 0.0347 - val_output_56_accuracy: 0.0343 - val_output_57_accuracy: 0.0363 - val_output_58_accuracy: 0.0391 - val_output_59_accuracy: 0.0356 - val_output_60_accuracy: 0.6624 - val_output_61_accuracy: 0.6358 - val_output_62_accuracy: 0.1128 - val_output_63_accuracy: 0.0754\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/41376 [..............................] - ETA: 8:34:52 - loss: 228.3732 - output_0_loss: 1.1921e-07 - output_1_loss: 1.1921e-07 - output_2_loss: 4.0089 - output_3_loss: 4.0924 - output_4_loss: 4.4826 - output_5_loss: 4.4654 - output_6_loss: 4.3444 - output_7_loss: 4.1900 - output_8_loss: 4.0336 - output_9_loss: 3.8898 - output_10_loss: 3.9298 - output_11_loss: 3.9665 - output_12_loss: 3.8927 - output_13_loss: 3.8229 - output_14_loss: 3.8019 - output_15_loss: 3.8593 - output_16_loss: 3.8520 - output_17_loss: 3.8629 - output_18_loss: 3.8552 - output_19_loss: 3.8885 - output_20_loss: 3.9031 - output_21_loss: 3.9810 - output_22_loss: 4.0189 - output_23_loss: 4.0956 - output_24_loss: 4.1426 - output_25_loss: 4.1414 - output_26_loss: 4.1959 - output_27_loss: 4.2625 - output_28_loss: 4.2756 - output_29_loss: 4.2755 - output_30_loss: 1.7757 - output_31_loss: 1.8820 - output_32_loss: 3.9489 - output_33_loss: 3.9174 - output_34_loss: 2.4818 - output_35_loss: 2.7193 - output_36_loss: 3.1699 - output_37_loss: 3.4532 - output_38_loss: 3.5759 - output_39_loss: 3.5368 - output_40_loss: 3.5607 - output_41_loss: 3.5620 - output_42_loss: 3.5325 - output_43_loss: 3.5082 - output_44_loss: 3.6166 - output_45_loss: 3.5714 - output_46_loss: 3.4964 - output_47_loss: 3.4336 - output_48_loss: 3.3337 - output_49_loss: 3.3837 - output_50_loss: 3.4799 - output_51_loss: 3.6148 - output_52_loss: 3.9027 - output_53_loss: 4.1360 - output_54_loss: 4.2323 - output_55_loss: 4.3043 - output_56_loss: 4.2951 - output_57_loss: 4.2689 - output_58_loss: 4.1763 - output_59_loss: 4.2952 - output_60_loss: 0.9613 - output_61_loss: 1.0941 - output_62_loss: 3.0368 - output_63_loss: 3.5843 - output_0_accuracy: 1.0000 - output_1_accuracy: 1.0000 - output_2_accuracy: 0.0879 - output_3_accuracy: 0.0732 - output_4_accuracy: 0.0098 - output_5_accuracy: 0.0264 - output_6_accuracy: 0.0234 - output_7_accuracy: 0.0352 - output_8_accuracy: 0.0352 - output_9_accuracy: 0.0557 - output_10_accuracy: 0.0234 - output_11_accuracy: 0.0352 - output_12_accuracy: 0.0361 - output_13_accuracy: 0.0576 - output_14_accuracy: 0.0557 - output_15_accuracy: 0.0645 - output_16_accuracy: 0.0654 - output_17_accuracy: 0.0674 - output_18_accuracy: 0.0771 - output_19_accuracy: 0.0566 - output_20_accuracy: 0.0605 - output_21_accuracy: 0.0557 - output_22_accuracy: 0.0479 - output_23_accuracy: 0.0410 - output_24_accuracy: 0.0371 - output_25_accuracy: 0.0400 - output_26_accuracy: 0.0391 - output_27_accuracy: 0.0264 - output_28_accuracy: 0.0254 - output_29_accuracy: 0.0264 - output_30_accuracy: 0.3711 - output_31_accuracy: 0.3037 - output_32_accuracy: 0.0361 - output_33_accuracy: 0.0469 - output_34_accuracy: 0.1455 - output_35_accuracy: 0.1377 - output_36_accuracy: 0.0957 - output_37_accuracy: 0.0713 - output_38_accuracy: 0.0566 - output_39_accuracy: 0.0674 - output_40_accuracy: 0.0840 - output_41_accuracy: 0.0908 - output_42_accuracy: 0.0850 - output_43_accuracy: 0.0967 - output_44_accuracy: 0.0732 - output_45_accuracy: 0.0693 - output_46_accuracy: 0.0928 - output_47_accuracy: 0.0918 - output_48_accuracy: 0.1094 - output_49_accuracy: 0.0947 - output_50_accuracy: 0.0947 - output_51_accuracy: 0.0781 - output_52_accuracy: 0.0430 - output_53_accuracy: 0.0410 - output_54_accuracy: 0.0293 - output_55_accuracy: 0.0381 - output_56_accuracy: 0.0361 - output_57_accuracy: 0.0498 - output_58_accuracy: 0.0469 - output_59_accuracy: 0.0273 - output_60_accuracy: 0.6230 - output_61_accuracy: 0.6045 - output_62_accuracy: 0.1123 - output_63_accuracy: 0.0908\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "    2/41376 [..............................] - ETA: 8:29:45 - loss: 228.6100 - output_0_loss: 1.1921e-07 - output_1_loss: 1.1921e-07 - output_2_loss: 4.1076 - output_3_loss: 4.2001 - output_4_loss: 4.4835 - output_5_loss: 4.4804 - output_6_loss: 4.3750 - output_7_loss: 4.2512 - output_8_loss: 4.0768 - output_9_loss: 3.9034 - output_10_loss: 3.9192 - output_11_loss: 3.9367 - output_12_loss: 3.9030 - output_13_loss: 3.8342 - output_14_loss: 3.7803 - output_15_loss: 3.8429 - output_16_loss: 3.8542 - output_17_loss: 3.8745 - output_18_loss: 3.8811 - output_19_loss: 3.8949 - output_20_loss: 3.8883 - output_21_loss: 3.9585 - output_22_loss: 4.0033 - output_23_loss: 4.0915 - output_24_loss: 4.1397 - output_25_loss: 4.1463 - output_26_loss: 4.1988 - output_27_loss: 4.2537 - output_28_loss: 4.2690 - output_29_loss: 4.2705 - output_30_loss: 1.8813 - output_31_loss: 2.0027 - output_32_loss: 3.9535 - output_33_loss: 3.9249 - output_34_loss: 2.5080 - output_35_loss: 2.7134 - output_36_loss: 3.1345 - output_37_loss: 3.4445 - output_38_loss: 3.5176 - output_39_loss: 3.5283 - output_40_loss: 3.5675 - output_41_loss: 3.5582 - output_42_loss: 3.5217 - output_43_loss: 3.5169 - output_44_loss: 3.5759 - output_45_loss: 3.5334 - output_46_loss: 3.4841 - output_47_loss: 3.4308 - output_48_loss: 3.3444 - output_49_loss: 3.3863 - output_50_loss: 3.4583 - output_51_loss: 3.5807 - output_52_loss: 3.8749 - output_53_loss: 4.1254 - output_54_loss: 4.2462 - output_55_loss: 4.3124 - output_56_loss: 4.3332 - output_57_loss: 4.2823 - output_58_loss: 4.1780 - output_59_loss: 4.2874 - output_60_loss: 0.9267 - output_61_loss: 1.0387 - output_62_loss: 3.0449 - output_63_loss: 3.5742 - output_0_accuracy: 1.0000 - output_1_accuracy: 1.0000 - output_2_accuracy: 0.0698 - output_3_accuracy: 0.0581 - output_4_accuracy: 0.0229 - output_5_accuracy: 0.0273 - output_6_accuracy: 0.0239 - output_7_accuracy: 0.0283 - output_8_accuracy: 0.0371 - output_9_accuracy: 0.0503 - output_10_accuracy: 0.0352 - output_11_accuracy: 0.0430 - output_12_accuracy: 0.0342 - output_13_accuracy: 0.0581 - output_14_accuracy: 0.0576 - output_15_accuracy: 0.0679 - output_16_accuracy: 0.0649 - output_17_accuracy: 0.0713 - output_18_accuracy: 0.0718 - output_19_accuracy: 0.0669 - output_20_accuracy: 0.0620 - output_21_accuracy: 0.0571 - output_22_accuracy: 0.0479 - output_23_accuracy: 0.0396 - output_24_accuracy: 0.0371 - output_25_accuracy: 0.0327 - output_26_accuracy: 0.0396 - output_27_accuracy: 0.0327 - output_28_accuracy: 0.0273 - output_29_accuracy: 0.0298 - output_30_accuracy: 0.3237 - output_31_accuracy: 0.2720 - output_32_accuracy: 0.0420 - output_33_accuracy: 0.0498 - output_34_accuracy: 0.1553 - output_35_accuracy: 0.1357 - output_36_accuracy: 0.1035 - output_37_accuracy: 0.0806 - output_38_accuracy: 0.0718 - output_39_accuracy: 0.0718 - output_40_accuracy: 0.0840 - output_41_accuracy: 0.0845 - output_42_accuracy: 0.0815 - output_43_accuracy: 0.0962 - output_44_accuracy: 0.0791 - output_45_accuracy: 0.0796 - output_46_accuracy: 0.0957 - output_47_accuracy: 0.0879 - output_48_accuracy: 0.1069 - output_49_accuracy: 0.1035 - output_50_accuracy: 0.0942 - output_51_accuracy: 0.0796 - output_52_accuracy: 0.0479 - output_53_accuracy: 0.0391 - output_54_accuracy: 0.0337 - output_55_accuracy: 0.0386 - output_56_accuracy: 0.0327 - output_57_accuracy: 0.0444 - output_58_accuracy: 0.0503 - output_59_accuracy: 0.0283 - output_60_accuracy: 0.6465 - output_61_accuracy: 0.6289 - output_62_accuracy: 0.1143 - output_63_accuracy: 0.0879"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  157/41376 [..............................] - ETA: 7:39:43 - loss: 229.1477 - output_0_loss: 1.2023e-07 - output_1_loss: 1.2019e-07 - output_2_loss: 4.0491 - output_3_loss: 4.1242 - output_4_loss: 4.4825 - output_5_loss: 4.4628 - output_6_loss: 4.3646 - output_7_loss: 4.2477 - output_8_loss: 4.0496 - output_9_loss: 3.9012 - output_10_loss: 3.8998 - output_11_loss: 3.9276 - output_12_loss: 3.8944 - output_13_loss: 3.8448 - output_14_loss: 3.8227 - output_15_loss: 3.8444 - output_16_loss: 3.8922 - output_17_loss: 3.9153 - output_18_loss: 3.9139 - output_19_loss: 3.9243 - output_20_loss: 3.9496 - output_21_loss: 3.9906 - output_22_loss: 4.0505 - output_23_loss: 4.1166 - output_24_loss: 4.1418 - output_25_loss: 4.1814 - output_26_loss: 4.2247 - output_27_loss: 4.2559 - output_28_loss: 4.2757 - output_29_loss: 4.2733 - output_30_loss: 1.8130 - output_31_loss: 1.9815 - output_32_loss: 3.9026 - output_33_loss: 3.9310 - output_34_loss: 2.5313 - output_35_loss: 2.7096 - output_36_loss: 3.1597 - output_37_loss: 3.4748 - output_38_loss: 3.5496 - output_39_loss: 3.5472 - output_40_loss: 3.6033 - output_41_loss: 3.6116 - output_42_loss: 3.5742 - output_43_loss: 3.5458 - output_44_loss: 3.5775 - output_45_loss: 3.5521 - output_46_loss: 3.5186 - output_47_loss: 3.4550 - output_48_loss: 3.3752 - output_49_loss: 3.3747 - output_50_loss: 3.4714 - output_51_loss: 3.6376 - output_52_loss: 3.8869 - output_53_loss: 4.1447 - output_54_loss: 4.2634 - output_55_loss: 4.3359 - output_56_loss: 4.3360 - output_57_loss: 4.2735 - output_58_loss: 4.2033 - output_59_loss: 4.2778 - output_60_loss: 0.8817 - output_61_loss: 0.9887 - output_62_loss: 3.0271 - output_63_loss: 3.6103 - output_0_accuracy: 1.0000 - output_1_accuracy: 1.0000 - output_2_accuracy: 0.0829 - output_3_accuracy: 0.0684 - output_4_accuracy: 0.0236 - output_5_accuracy: 0.0224 - output_6_accuracy: 0.0246 - output_7_accuracy: 0.0282 - output_8_accuracy: 0.0390 - output_9_accuracy: 0.0502 - output_10_accuracy: 0.0433 - output_11_accuracy: 0.0421 - output_12_accuracy: 0.0469 - output_13_accuracy: 0.0536 - output_14_accuracy: 0.0596 - output_15_accuracy: 0.0616 - output_16_accuracy: 0.0637 - output_17_accuracy: 0.0661 - output_18_accuracy: 0.0670 - output_19_accuracy: 0.0616 - output_20_accuracy: 0.0564 - output_21_accuracy: 0.0504 - output_22_accuracy: 0.0454 - output_23_accuracy: 0.0406 - output_24_accuracy: 0.0395 - output_25_accuracy: 0.0369 - output_26_accuracy: 0.0348 - output_27_accuracy: 0.0334 - output_28_accuracy: 0.0320 - output_29_accuracy: 0.0311 - output_30_accuracy: 0.3429 - output_31_accuracy: 0.2892 - output_32_accuracy: 0.0504 - output_33_accuracy: 0.0496 - output_34_accuracy: 0.1594 - output_35_accuracy: 0.1422 - output_36_accuracy: 0.1034 - output_37_accuracy: 0.0849 - output_38_accuracy: 0.0775 - output_39_accuracy: 0.0769 - output_40_accuracy: 0.0739 - output_41_accuracy: 0.0780 - output_42_accuracy: 0.0768 - output_43_accuracy: 0.0793 - output_44_accuracy: 0.0758 - output_45_accuracy: 0.0806 - output_46_accuracy: 0.0853 - output_47_accuracy: 0.0934 - output_48_accuracy: 0.1028 - output_49_accuracy: 0.1017 - output_50_accuracy: 0.0915 - output_51_accuracy: 0.0768 - output_52_accuracy: 0.0585 - output_53_accuracy: 0.0432 - output_54_accuracy: 0.0379 - output_55_accuracy: 0.0348 - output_56_accuracy: 0.0345 - output_57_accuracy: 0.0367 - output_58_accuracy: 0.0389 - output_59_accuracy: 0.0353 - output_60_accuracy: 0.6638 - output_61_accuracy: 0.6365 - output_62_accuracy: 0.1133 - output_63_accuracy: 0.0763"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c7a941bd8397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mNep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     model.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n\u001b[0;32m----> 4\u001b[0;31m                   callbacks=[earlyStopping, mcp_save])\n\u001b[0m",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Keep track of the stateful output loss result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0moutput_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0;31m# Scale output loss for distribution. For custom losses we assume\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m  \u001b[0;31m# pylint:disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     return distributed_training_utils.call_replica_local_fn(\n\u001b[0;32m--> 193\u001b[0;31m         replica_local_fn, *args, **kwargs)\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36mcall_replica_local_fn\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36mreplica_local_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplica_local_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;34m\"\"\"Updates the state of the metric in a replica-local context.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# update_op will be None in eager execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, values, sample_weight)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWEIGHTED_MEAN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mnum_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mnum_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m    702\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/home/ankitesg/anaconda3/envs/CBrainEnv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   2197\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cast\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DstT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDstT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Truncate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2199\u001b[0;31m         Truncate)\n\u001b[0m\u001b[1;32m   2200\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2201\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    Nep = 5\n",
    "    model.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n",
    "                  callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:2', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RH, T-TNS Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,))\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "densout = Dense(64*(bin_size+2), activation='sigmoid')(densout)\n",
    "model = tf.keras.models.Model(inp, densout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CBrainEnv",
   "language": "python",
   "name": "cbrainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
