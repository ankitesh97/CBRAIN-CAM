{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bd17a52d7a27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDataGeneratorClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, data_fn, input_vars, output_vars, percentile_path, data_name,\n\u001b[1;32m      3\u001b[0m                  \u001b[0mnorm_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  batch_size=1024, shuffle=True, xarray=False, var_cut_off=None, normalize_flag=True, bin_size=100):\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Just copy over the attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class DataGeneratorClassification(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data_fn, input_vars, output_vars, percentile_path, data_name,\n",
    "                 norm_fn=None, input_transform=None, output_transform=None,\n",
    "                 batch_size=1024, shuffle=True, xarray=False, var_cut_off=None, normalize_flag=True, bin_size=100):\n",
    "        # Just copy over the attributes\n",
    "        self.data_fn, self.norm_fn = data_fn, norm_fn\n",
    "        self.input_vars, self.output_vars = input_vars, output_vars\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "        self.bin_size = bin_size\n",
    "        self.percentile_bins = load_pickle(percentile_path)['Percentile'][data_name]\n",
    "        self.enc = OneHotEncoder(sparse=False)\n",
    "        classes = np.arange(self.bin_size+2)\n",
    "        self.enc.fit(classes.reshape(-1,1))\n",
    "        # Open datasets\n",
    "        self.data_ds = xr.open_mfdataset(data_fn)\n",
    "        if norm_fn is not None: self.norm_ds = xr.open_dataset(norm_fn)\n",
    "     # Compute number of samples and batches\n",
    "        self.n_samples = self.data_ds.X.shape[0]\n",
    "        self.n_batches = int(np.floor(self.n_samples) / self.batch_size)\n",
    "\n",
    "        self.n_inputs, self.n_outputs = 64, 64\n",
    "        \n",
    "                # Initialize input and output normalizers/transformers\n",
    "        if input_transform is None:\n",
    "            self.input_transform = Normalizer()\n",
    "        elif type(input_transform) is tuple:\n",
    "            ## normalize flag added by Ankitesh\n",
    "            self.input_transform = InputNormalizer(\n",
    "                self.norm_ds,normalize_flag, input_vars, input_transform[0], input_transform[1], var_cut_off)\n",
    "        else:\n",
    "            self.input_transform = input_transform  # Assume an initialized normalizer is passed\n",
    "            \n",
    "            \n",
    "        if output_transform is None:\n",
    "            self.output_transform = Normalizer()\n",
    "        elif type(output_transform) is dict:\n",
    "            self.output_transform = DictNormalizer(self.norm_ds, output_vars, output_transform)\n",
    "        else:\n",
    "            self.output_transform = output_transform  # Assume an initialized normalizer is passed\n",
    "\n",
    "        # Now close the xarray file and load it as an h5 file instead\n",
    "        # This significantly speeds up the reading of the data...\n",
    "#         if not xarray:\n",
    "#             self.data_ds.close()\n",
    "#             self.data_ds = h5py.File(data_fn, 'r')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "     \n",
    "    def __getitem__(self, index):\n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch_X = self.data_ds['X'][start_idx:end_idx].values\n",
    "        Y = self.data_ds['Y'][start_idx:end_idx].values\n",
    "        # Normalize\n",
    "        X = self.input_transform.transform(batch_X)\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(self.n_batches)\n",
    "        if self.shuffle: np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CBrainEnv",
   "language": "python",
   "name": "cbrainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
